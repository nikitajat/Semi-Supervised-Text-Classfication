{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pfcTvYKIn2X",
        "outputId": "f2b56967-8d9e-4529-c961-01954b23ee1e"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtaX75TJI0ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0635306c-3f18-4a6e-803a-5c6b4b5fbe2f"
      },
      "source": [
        "##### For IMDB dataset #######\r\n",
        "#!unzip /content/IMDB\\ Dataset.csv.zip -d /content\r\n",
        "'''df = pd.read_csv(\"/content/drive/MyDrive/nlp datasets/IMDB Dataset.csv\")\r\n",
        "print(df.head())\r\n",
        "def label_to_int(label):\r\n",
        "  if label == \"positive\":\r\n",
        "    return 1\r\n",
        "  else:\r\n",
        "    return  0\r\n",
        "\r\n",
        "df['sentiment'] = df['sentiment'].apply(label_to_int)\r\n",
        "n_classes = 2 '''\r\n",
        "\r\n",
        "##### For AGNEWS dataset\r\n",
        "'''\r\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/nlp datasets/AGNEWS.csv\")\r\n",
        "print(df.head())\r\n",
        "print(np.unique(df['sentiment']))\r\n",
        "def label_(label):\r\n",
        "    return  label-1\r\n",
        "df['sentiment'] = df['sentiment'].apply(label_)\r\n",
        "print(np.unique(df['sentiment']))\r\n",
        "n_classes = 4'''\r\n",
        "\r\n",
        "##### For TREC-6 dataset\r\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/nlp datasets/TREC-6.csv\")\r\n",
        "print(df.head())\r\n",
        "print(np.unique(df['sentiment']))\r\n",
        "def label_to_int(label):\r\n",
        "  if label == 'ABBR':\r\n",
        "    return 0\r\n",
        "  if label == 'DESC':\r\n",
        "    return 1\r\n",
        "  if label == 'ENTY':\r\n",
        "    return 2\r\n",
        "  if label == 'HUM':\r\n",
        "    return 3\r\n",
        "  if label == 'LOC':\r\n",
        "    return 4\r\n",
        "  if label == 'NUM':\r\n",
        "    return 5\r\n",
        "\r\n",
        "df['sentiment'] = df['sentiment'].apply(label_to_int)\r\n",
        "n_classes = 6\r\n",
        "print(np.unique(df['sentiment']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0                                             review sentiment\n",
            "0           0  How did serfdom develop in and then leave Russ...      DESC\n",
            "1           1  What films featured the character Popeye Doyle...      ENTY\n",
            "2           2  How can I find a list of celebrities ' real na...      DESC\n",
            "3           3  What fowl grabs the spotlight after the Chines...      ENTY\n",
            "4           4                What is the full form of .com ?\\r\\n      ABBR\n",
            "['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
            "[0 1 2 3 4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66J3gA3iJT-G",
        "outputId": "267dec7b-2ef9-42af-e3bd-c63e88f2abca"
      },
      "source": [
        "test_size = 0.1\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "corpus_train, corpus_test = train_test_split(df , test_size=test_size, stratify=df['sentiment'])\r\n",
        "corpus_train, corpus_dev = train_test_split(corpus_train, train_size=.9, stratify=corpus_train['sentiment'])\r\n",
        "corpus_train, corpus_ul = train_test_split(corpus_train , train_size=200, stratify=corpus_train['sentiment'])\r\n",
        "print(len(corpus_train))\r\n",
        "print(len(corpus_dev))\r\n",
        "print(len(corpus_ul))\r\n",
        "print(len(corpus_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "491\n",
            "4215\n",
            "546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdD3MSh7JX-z"
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\r\n",
        "stop_words = nltk.corpus.stopwords.words('english')\r\n",
        "def normalize_document(doc):\r\n",
        "\r\n",
        "    # lower case and remove special characters\\whitespaces\r\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\r\n",
        "    doc = doc.lower()\r\n",
        "    doc = doc.strip()\r\n",
        "    doc = doc.replace('\"',\"\")\r\n",
        "\r\n",
        "    # tokenize document\r\n",
        "    tokens = wpt.tokenize(doc)\r\n",
        "\r\n",
        "    # filter stopwords out of document\r\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\r\n",
        "\r\n",
        "    # re-create document from filtered tokens\r\n",
        "    return \" \".join(filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_s3BpCoJcZg"
      },
      "source": [
        "#corpus_dev['review'] = corpus_dev['review'].apply(normalize_document)\r\n",
        "#corpus_train['review'] = corpus_train['review'].apply(normalize_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D0aAu7rLlW1E",
        "outputId": "a1f3480d-e0ea-47a5-c412-92747b3e2c70"
      },
      "source": [
        "corpus_dev['review'].iloc[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"How long was Mao 's 1930s Long March ?\\r\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "lKoPessGJms3",
        "outputId": "c3da4a82-ff2d-46ac-caca-874aa214b422"
      },
      "source": [
        "seq_len = [len(i.split()) for i in corpus_ul['review']]\r\n",
        "pd.Series(seq_len).hist(bins = 7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc27412e4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD30oaWkO50e",
        "outputId": "40a139dd-0b9c-4613-e22d-7367d8d6af4a"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5lZeqqTJqiv"
      },
      "source": [
        "import transformers\r\n",
        "from transformers import AdamW\r\n",
        "from transformers import BertModel, BertTokenizer\r\n",
        "\r\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\r\n",
        "# Load the BERT tokenizer\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "MAX_TOKENS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkvyj0SDwGNx"
      },
      "source": [
        "tr = list(corpus_train['review'])\r\n",
        "vl = list(corpus_dev['review'])\r\n",
        "ul = list(corpus_ul['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr8TT55LmPnY",
        "outputId": "b4b7ce1f-3c47-4190-9bfe-730ed6bd30d7"
      },
      "source": [
        "print(vl[5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What dummy received an honorary degree from Northwestern University ?\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AySHSkW5J0dr"
      },
      "source": [
        "tokens_train = tokenizer(\r\n",
        "    tr,\r\n",
        "    padding = True,\r\n",
        "    max_length = MAX_TOKENS,\r\n",
        "    truncation=True\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the validation set\r\n",
        "tokens_dev = tokenizer(\r\n",
        "    vl,\r\n",
        "    padding = True,\r\n",
        "    max_length = MAX_TOKENS,\r\n",
        "    truncation=True\r\n",
        ")\r\n",
        "\r\n",
        "tokens_ul = tokenizer(\r\n",
        "    ul,\r\n",
        "    padding = True,\r\n",
        "    max_length = MAX_TOKENS,\r\n",
        "    truncation=True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX0ygTbkyaxG",
        "outputId": "9f83139b-6f61-4872-bccb-17215de43444"
      },
      "source": [
        "print(tokenizer.decode(tokens_ul['input_ids'][5]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] what european country boasts the city of furth, found where the rivers rednitz and peg [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3tfqMUutnG9",
        "outputId": "b3e7fbd9-facf-4485-cf61-e7243d492d01"
      },
      "source": [
        "print(len(tokens_train['input_ids'][0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnXJmvcAkEdk",
        "outputId": "7be64cc3-a8b9-40d2-a995-dc331c1766fe"
      },
      "source": [
        "a = list(corpus_train['sentiment'])\r\n",
        "b = list(corpus_dev['sentiment'])\r\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 1, 1, 1, 1, 2, 2, 2, 1, 4, 3, 3, 3, 3, 4, 1, 5, 1, 3, 5, 4, 2, 1, 5, 4, 3, 1, 5, 5, 2, 3, 2, 2, 5, 2, 4, 1, 3, 5, 5, 1, 4, 3, 1, 5, 0, 3, 1, 3, 3, 2, 1, 2, 2, 1, 4, 3, 5, 5, 3, 4, 2, 4, 5, 2, 4, 5, 5, 5, 4, 1, 5, 3, 4, 4, 5, 4, 3, 4, 5, 1, 1, 5, 2, 1, 3, 2, 2, 4, 3, 1, 4, 4, 4, 5, 3, 2, 2, 3, 3, 4, 2, 1, 2, 3, 1, 1, 1, 3, 3, 3, 3, 3, 2, 4, 0, 4, 2, 3, 3, 2, 3, 3, 1, 2, 2, 5, 2, 5, 3, 2, 5, 3, 3, 1, 2, 2, 1, 1, 2, 2, 3, 5, 3, 4, 3, 1, 0, 3, 1, 1, 1, 1, 4, 5, 1, 1, 4, 1, 2, 2, 2, 2, 5, 3, 4, 2, 1, 2, 4, 4, 4, 5, 1, 5, 5, 3, 1, 1, 2, 3, 1, 2, 5, 2, 5, 4, 3, 2, 3, 3, 1, 5, 2, 2, 2, 1, 2, 5, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0d05iBrKACo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f80901-588f-4513-9a35-077121307549"
      },
      "source": [
        "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
        "train_y = torch.tensor(a)\r\n",
        "\r\n",
        "dev_seq = torch.tensor(tokens_dev['input_ids'])\r\n",
        "dev_mask = torch.tensor(tokens_dev['attention_mask'])\r\n",
        "dev_y = torch.tensor(b)\r\n",
        "\r\n",
        "ul_seq = torch.tensor(tokens_ul['input_ids'])\r\n",
        "ul_mask = torch.tensor(tokens_ul['attention_mask'])\r\n",
        "\r\n",
        "print(dev_seq.shape)\r\n",
        "print(train_seq.shape)\r\n",
        "print(ul_seq.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([491, 20])\n",
            "torch.Size([200, 20])\n",
            "torch.Size([4215, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6POONGtCfo6W",
        "outputId": "8124df24-8546-481e-ca5f-1bf1ab6f4358"
      },
      "source": [
        "print(ul_seq.shape)\r\n",
        "print(ul_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4215, 20])\n",
            "torch.Size([4215, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAThuyzBKHFD"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "#define a batch size\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
        "# sampler for sampling the data during training\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "# dataLoader for train set\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "dev_data = TensorDataset(dev_seq, dev_mask, dev_y)\r\n",
        "# sampler for sampling the data during training\r\n",
        "dev_sampler = SequentialSampler(dev_data)\r\n",
        "# dataLoader for validation set\r\n",
        "dev_dataloader = DataLoader(dev_data, sampler = dev_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "ul_data = TensorDataset(ul_seq, ul_mask)\r\n",
        "# sampler for sampling the data during training\r\n",
        "ul_sampler = RandomSampler(ul_data)\r\n",
        "# dataLoader for train set\r\n",
        "ul_dataloader = DataLoader(ul_data, sampler=ul_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSryJL9dIqp"
      },
      "source": [
        "for param in bert.parameters():\r\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y_saTeLJQzo"
      },
      "source": [
        "class Encoder_Classifier(nn.Module):\r\n",
        "    def __init__(self, bert, n_classes):\r\n",
        "      super(Encoder_Classifier, self).__init__()\r\n",
        "      self.bert = bert\r\n",
        "      \r\n",
        "      # dropout layer\r\n",
        "      self.dropout = nn.Dropout(0.1)\r\n",
        "      \r\n",
        "      # relu activation function\r\n",
        "      self.relu =  nn.ReLU()\r\n",
        "\r\n",
        "      # dense layer 1\r\n",
        "      self.fc1 = nn.Linear(768,n_classes)\r\n",
        "\r\n",
        "      #softmax activation function\r\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "\r\n",
        "      #pass the inputs to the model  \r\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\r\n",
        "      \r\n",
        "      \r\n",
        "      x = self.fc1(cls_hs) \r\n",
        "      \r\n",
        "      # apply softmax activation\r\n",
        "      x = self.softmax(x)\r\n",
        "\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZdPYAwjaMrm"
      },
      "source": [
        "def train(weight):\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  total_celoss, total_mseloss, total_accuracy = 0, 0,0\r\n",
        "  '''\r\n",
        "  # empty list to save model predictions\r\n",
        "  total_preds=[]'''\r\n",
        "  \r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    #batch = [r.to(device) for r in batch]\r\n",
        "    \r\n",
        "    ###### for  labeled data, computing cross entropy   #########\r\n",
        "    sent_id, mask, labels = batch[0].to(device),batch[1].to(device),batch[2].to(device)\r\n",
        "\r\n",
        "    model.zero_grad()        \r\n",
        "    preds = model(sent_id, mask)\r\n",
        "    celoss = CELoss(preds, labels)\r\n",
        "\r\n",
        "\r\n",
        "    _,batch = next(enum_unlabeled_loader)\r\n",
        "    sent_id, mask =  batch[0].to(device),batch[1].to(device)\r\n",
        "    preds1 = model(sent_id, mask)\r\n",
        "    preds2 = model(sent_id, mask)\r\n",
        "\r\n",
        "    mseloss = MSELoss(preds1,preds2)\r\n",
        "\r\n",
        "    # backward pass to calculate the gradients\r\n",
        "    finalloss = celoss + weight * mseloss\r\n",
        "    finalloss.backward()\r\n",
        "\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "    # update parameters\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # add on to the total loss\r\n",
        "    loss_item = finalloss.item()\r\n",
        "\r\n",
        "    # progress update after every 5 batches.\r\n",
        "    if step % 5 == 0 and not step == 0:\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n",
        "      print(\"loss\",loss_item)\r\n",
        "\r\n",
        "    total_celoss = total_celoss + celoss.item()\r\n",
        "    total_mseloss = total_mseloss + mseloss.item()\r\n",
        "\r\n",
        "    '''\r\n",
        "    # model predictions are stored on GPU. So, push it to CPU\r\n",
        "    preds=preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "    # append the model predictions\r\n",
        "    total_preds.append(preds)'''\r\n",
        "\r\n",
        "  # compute the training loss of the epoch\r\n",
        "  avg_celoss = total_celoss / len(train_dataloader)\r\n",
        "  avg_mseloss = total_mseloss / len(train_dataloader)\r\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  #total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  #returns the loss and predictions\r\n",
        "  return avg_celoss, avg_mseloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzIX9d9PahqL"
      },
      "source": [
        "def evaluate():\r\n",
        "  \r\n",
        "  print(\"\\nEvaluating...\")\r\n",
        "  \r\n",
        "  # deactivate dropout layers\r\n",
        "  model.eval()\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  total_preds = []\r\n",
        "  count=0\r\n",
        "\r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(dev_dataloader):\r\n",
        "    \r\n",
        "    # Progress update every 50 batches.\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      \r\n",
        "            \r\n",
        "      # Report progress.\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dev_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    #batch = [t.to(device) for t in batch]\r\n",
        "    \r\n",
        "\r\n",
        "    sent_id, mask, labels = batch[0].to(device),batch[1].to(device),batch[2].to(device)\r\n",
        "\r\n",
        "    # deactivate autograd\r\n",
        "    with torch.no_grad():\r\n",
        "      \r\n",
        "      # model predictions\r\n",
        "      preds = model(sent_id, mask)\r\n",
        "\r\n",
        "      # compute the validation loss between actual and predicted values\r\n",
        "      loss = CELoss(preds,labels)\r\n",
        "\r\n",
        "      total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "      preds = preds.detach().cpu().numpy()\r\n",
        "      total_preds.append(preds)\r\n",
        "\r\n",
        "      true_class = np.argmax(preds,axis=1)\r\n",
        "      for myvar in range(len(labels)):\r\n",
        "        if labels[myvar]== true_class[myvar]:\r\n",
        "          count+=1\r\n",
        "\r\n",
        "  # compute the validation loss of the epoch\r\n",
        "  avg_loss = total_loss / len(dev_dataloader) \r\n",
        "\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "  print(\"Validation accuracy\",count/len(dev_data))\r\n",
        "\r\n",
        "  return avg_loss, count/len(dev_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQXkbXtjpS5N"
      },
      "source": [
        "best_valid_loss = float('inf')\r\n",
        "train_losses = []\r\n",
        "valid_losses = []\r\n",
        "accuracy_list = []\r\n",
        "learning_rate = .0005\r\n",
        "freezed_epochs = 15\r\n",
        "n_layer_unfreeze = 2\r\n",
        "weight = .3\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "CELoss = nn.CrossEntropyLoss()\r\n",
        "MSELoss = nn.MSELoss()\r\n",
        "model = Encoder_Classifier(bert, n_classes).to(device)\r\n",
        "optimizer = AdamW(model.parameters(),lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ygsRjWta7ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd05cbfb-ed35-4487-9644-05649f81de07"
      },
      "source": [
        "\r\n",
        "#for each epoch\r\n",
        "for epoch in range(freezed_epochs):\r\n",
        "     \r\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, freezed_epochs))\r\n",
        "    \r\n",
        "    #train model\r\n",
        "    enum_unlabeled_loader = enumerate(ul_dataloader)\r\n",
        "    train_loss = train(weight)\r\n",
        "    \r\n",
        "    #evaluate model\r\n",
        "    valid_loss, accuracy = evaluate()\r\n",
        "    \r\n",
        "    #save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    # append training and validation loss\r\n",
        "    train_losses.append(train_loss)\r\n",
        "    valid_losses.append(valid_loss)\r\n",
        "    accuracy_list.append(accuracy)\r\n",
        "\r\n",
        "    print(f'\\nTraining Loss: {train_loss[0]:.3f}')\r\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\r\n",
        "\r\n",
        "##### unfreezing layers #######\r\n",
        "for iter in range(n_layer_unfreeze):\r\n",
        "    print(str(iter+1)+\" unfreeze\")\r\n",
        "    for param in model.bert.encoder.layer._modules[str(11-iter)].parameters():\r\n",
        "        param.requires_grad=True\r\n",
        "    for epoch in range(2):\r\n",
        "     \r\n",
        "        print('\\n Epoch {:} / {:}'.format(epoch + 1, 2))\r\n",
        "        \r\n",
        "        #train model\r\n",
        "        \r\n",
        "        enum_unlabeled_loader = enumerate(ul_dataloader)\r\n",
        "        train_loss = train(weight)\r\n",
        "        \r\n",
        "        #evaluate model\r\n",
        "        valid_loss, accuracy = evaluate()\r\n",
        "        \r\n",
        "        #save the best model\r\n",
        "        if valid_loss < best_valid_loss:\r\n",
        "            best_valid_loss = valid_loss\r\n",
        "            torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "        \r\n",
        "        # append training and validation loss\r\n",
        "        train_losses.append(train_loss)\r\n",
        "        valid_losses.append(valid_loss)\r\n",
        "        accuracy_list.append(accuracy)\r\n",
        "        \r\n",
        "        print(f'\\nTraining Loss: {train_loss[0]:.3f}')\r\n",
        "        print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.20162932790224034\n",
            "\n",
            "Training Loss: 1.753\n",
            "Validation Loss: 1.699\n",
            "\n",
            " Epoch 2 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.219959266802444\n",
            "\n",
            "Training Loss: 1.671\n",
            "Validation Loss: 1.680\n",
            "\n",
            " Epoch 3 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.19959266802443992\n",
            "\n",
            "Training Loss: 1.651\n",
            "Validation Loss: 1.647\n",
            "\n",
            " Epoch 4 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.219959266802444\n",
            "\n",
            "Training Loss: 1.625\n",
            "Validation Loss: 1.658\n",
            "\n",
            " Epoch 5 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.2240325865580448\n",
            "\n",
            "Training Loss: 1.626\n",
            "Validation Loss: 1.658\n",
            "\n",
            " Epoch 6 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.26883910386965376\n",
            "\n",
            "Training Loss: 1.624\n",
            "Validation Loss: 1.635\n",
            "\n",
            " Epoch 7 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.28716904276985744\n",
            "\n",
            "Training Loss: 1.586\n",
            "Validation Loss: 1.629\n",
            "\n",
            " Epoch 8 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.27494908350305497\n",
            "\n",
            "Training Loss: 1.602\n",
            "Validation Loss: 1.626\n",
            "\n",
            " Epoch 9 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.285132382892057\n",
            "\n",
            "Training Loss: 1.697\n",
            "Validation Loss: 1.620\n",
            "\n",
            " Epoch 10 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.2892057026476578\n",
            "\n",
            "Training Loss: 1.601\n",
            "Validation Loss: 1.618\n",
            "\n",
            " Epoch 11 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.2953156822810591\n",
            "\n",
            "Training Loss: 1.615\n",
            "Validation Loss: 1.617\n",
            "\n",
            " Epoch 12 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.3604887983706721\n",
            "\n",
            "Training Loss: 1.671\n",
            "Validation Loss: 1.612\n",
            "\n",
            " Epoch 13 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.35845213849287166\n",
            "\n",
            "Training Loss: 1.589\n",
            "Validation Loss: 1.607\n",
            "\n",
            " Epoch 14 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.2973523421588595\n",
            "\n",
            "Training Loss: 1.607\n",
            "Validation Loss: 1.606\n",
            "\n",
            " Epoch 15 / 15\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.2973523421588595\n",
            "\n",
            "Training Loss: 1.594\n",
            "Validation Loss: 1.603\n",
            "1 unfreeze\n",
            "\n",
            " Epoch 1 / 2\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.5661914460285132\n",
            "\n",
            "Training Loss: 1.542\n",
            "Validation Loss: 1.294\n",
            "\n",
            " Epoch 2 / 2\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.5010183299389002\n",
            "\n",
            "Training Loss: 1.205\n",
            "Validation Loss: 1.253\n",
            "2 unfreeze\n",
            "\n",
            " Epoch 1 / 2\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.5580448065173116\n",
            "\n",
            "Training Loss: 1.191\n",
            "Validation Loss: 1.102\n",
            "\n",
            " Epoch 2 / 2\n",
            "\n",
            "Evaluating...\n",
            "Validation accuracy 0.6659877800407332\n",
            "\n",
            "Training Loss: 0.796\n",
            "Validation Loss: 0.921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4NHuM9c2kiI"
      },
      "source": [
        "np.savetxt('/content/drive/MyDrive/nlp datasets/TREC-6_valloss.txt',valid_losses,delimiter = \",\")\r\n",
        "np.savetxt('/content/drive/MyDrive/nlp datasets/TREC-6_accuracy.txt',accuracy_list,delimiter = \",\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}